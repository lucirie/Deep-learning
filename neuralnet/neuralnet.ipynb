{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8a6ebe3",
   "metadata": {},
   "source": [
    "# Neural Network for Image Classification with Numpy\n",
    "\n",
    "In this notebook, I'll be making a Neural network for image classification on the MNIST Dataset. A dataset containing handwritten digits of 60,000 training examples and 10,000 testing examples.\n",
    "\n",
    "Goals:\n",
    "- [Loading Data and Initializing Parameters](#loading-the-data)\n",
    "- [Forward Propagation](#forward-propagation)\n",
    "- [Backward Propagation](#backward-propagation)\n",
    "- [Testing the model](#testing-the-model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115bbb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import helper as mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c79c867",
   "metadata": {},
   "source": [
    "## Loading The Data\n",
    "\n",
    "A set of very usual helper functions provided by MNIST is available in helper.py containing the functions needed to load, and view the images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f014fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_images, training_labels = 'train-images.idx3-ubyte', 'train-labels.idx1-ubyte'\n",
    "testing_images, testing_labels = 't10k-images.idx3-ubyte', 't10k-labels.idx1-ubyte'\n",
    "data = mnist.MnistDataloader(training_images, training_labels, testing_images, testing_labels) # Load data using helper function\n",
    "(x_train, y_train), (x_test, y_test) = data.load_data()\n",
    "\n",
    "## For visualizing the dataset ##\n",
    "test_list, title_list = [], []\n",
    "for i in range(1, 11):\n",
    "    r = np.random.randint(1, 10000)\n",
    "    test_list.append(x_train[r])\n",
    "    title_list.append(f'Training example {r}, Label: {str(y_train[r])}')\n",
    "\n",
    "mnist.show_images(test_list, title_list) # Helper function to view images\n",
    "\n",
    "# Converting to numpy arrays\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], -1).T # Reshaping x_train to correct shape of (784, 60000) for correct parameter initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7160b7",
   "metadata": {},
   "source": [
    "## Initializing Parameters\n",
    "\n",
    "In a deep neural network, the dimensions of parameters for each layer is: (dimension of current layer, dimension of previous layer).\n",
    "I'll create a function that takes input the dimensions of the neural network, and outputs a list of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb21a3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(architecture):\n",
    "    np.random.seed(42)\n",
    "    parameters = {}\n",
    "    for l in range(1, len(architecture)):\n",
    "        parameters[\"W\" + str(l)] = np.random.rand(architecture[l-1][\"units\"], architecture[l][\"units\"]) * 0.01 # (current layer dimension, previous layer domimension)\n",
    "        parameters[\"b\" + str(l)] = np.zeros((architecture[l][\"units\"], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57a9656",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "This network will be 7 layers repeating a \"linear --> relu activation\" block 3 times, and a final softmax output to otput ${\\hat{y}}$ all the while saving the parameters W, b and the previous Activations as cache for back-propagation when training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d370360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(A_prev, W, b):\n",
    "    return W.T @ A_prev + b\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "    softmax =  exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "    return softmax\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58393e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, architecture):\n",
    "    cache = {}\n",
    "    A = X\n",
    "    for l in range(1, len(architecture)):\n",
    "        A_prev = A\n",
    "        W = parameters[\"W\" + str(l)]\n",
    "        b = parameters[\"b\" + str(l)]\n",
    "        activation = architecture[l]['activation']\n",
    "\n",
    "        Z = linear(A_prev, W, b)\n",
    "        if activation == \"linear\":\n",
    "            A = Z\n",
    "        elif activation == \"relu\":\n",
    "            A = relu(Z)\n",
    "        elif activation == \"softmax\":\n",
    "            A = softmax(Z)\n",
    "        cache[l] = {\n",
    "            \"A\": A,\n",
    "            \"W\": W,\n",
    "            \"Z\": Z\n",
    "        }\n",
    "    cache[0] = {\"A\": X}\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bbf580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(architecture, cache, y):\n",
    "    L = len(architecture) - 1\n",
    "    m = y.shape[1]\n",
    "    gradients = {}\n",
    "    # Output layer\n",
    "    gradients[\"dZ\" + str(L)] = cache[L][\"A\"] - y\n",
    "    gradients[\"dW\" + str(L)] = gradients[\"dZ\" + str(L)] @ cache[L - 1][\"A\"].T / m\n",
    "    gradients[\"db\" + str(L)] = np.sum(gradients[\"dZ\" + str(L)], axis=1, keepdims=True) / m\n",
    "\n",
    "    # Each layer\n",
    "    for l in reversed(range(1, L)):\n",
    "        gradients[\"dA\" + str(l)] = cache[l + 1][\"W\"] @ gradients[\"dZ\" + str(l + 1)] \n",
    "        gradients[\"dZ\" + str(l)] = gradients[\"dA\" + str(l)] * relu_derivative(cache[l][\"Z\"])\n",
    "        gradients[\"dW\" + str(l)] = gradients[\"dZ\" + str(l)] @ cache[l - 1][\"A\"].T / m\n",
    "        gradients[\"db\" + str(l)] = np.sum(gradients[\"dZ\" + str(l)], axis=1, keepdims=True) / m\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab535282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(gradients, parameters, architecture, learning_rate=0.01):\n",
    "    L = len(architecture) - 1\n",
    "    for l in reversed(range(1, L)):\n",
    "        parameters[\"W\" + str(l)] -= learning_rate * gradients[\"dW\" + str(l)].T\n",
    "        parameters[\"b\" + str(l)] -= learning_rate * gradients[\"db\" + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5d8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(architecture, x, y, learning_rate=0.01, itterations=100):\n",
    "    parameters = initialize_parameters_deep(architecture)\n",
    "    A, cache = forward_propagation(x, parameters, architecture)\n",
    "    for i in range(itterations):\n",
    "        gradients = calculate_gradients(architecture, cache, y)\n",
    "        parameters = back_propagation(gradients, parameters, architecture, learning_rate)\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c5849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, parameters, architecture):\n",
    "    A, cache = forward_propagation(x, parameters, architecture)\n",
    "    return A[len(architecture)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95237b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y):\n",
    "    y_hot = np.zeros((y.size, y.max() + 1))\n",
    "    y_hot[np.arange(y.size), y] = 1\n",
    "    return y_hot.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66bf65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "architecture = [\n",
    "    {\"units\": x_train.shape[0]},\n",
    "    {\"units\": 16, \"activation\": \"linear\"},\n",
    "    {\"units\": 16, \"activation\": \"relu\"},\n",
    "    {\"units\": 16, \"activation\": \"linear\"},\n",
    "    {\"units\": 16, \"activation\": \"relu\"},\n",
    "    {\"units\": 16, \"activation\": \"linear\"},\n",
    "    {\"units\": 16, \"activation\": \"relu\"},\n",
    "    {\"units\": 16, \"activation\": \"linear\"},\n",
    "    {\"units\": 16, \"activation\": \"relu\"},\n",
    "    {\"units\": 10,  \"activation\": \"softmax\"}\n",
    "]\n",
    "y_train = one_hot(y_train)\n",
    "parameters = fit_model(architecture, x_train, y_train, learning_rate=0.01, itterations=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
